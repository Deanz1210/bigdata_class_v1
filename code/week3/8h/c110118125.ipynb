{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓三個種類新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links =['index.php?k=1', 'index.php?k=5', 'index.php?k=13','index.php?k=11']\n",
    "news_categories=['PC','動漫畫','電競','活動展覽']\n",
    "base_url = 'https://gnn.gamer.com.tw/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (iPhone; CPU iPhone OS 18_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.3 Mobile/15E148 Safari/604.1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent = UserAgent()\n",
    "user_agent.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存放資料之變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "dates = []\n",
    "contents = []\n",
    "categories = []\n",
    "item_id = []\n",
    "photo_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting categorical news: PC\n",
      "https://gnn.gamer.com.tw/index.php?k=1\n",
      "1 -- 多平台\n",
      "網石公開全新開放世界收集型 RPG《七大罪：起源》官方預告網站\n",
      " 9 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282748\n",
      "2 -- 多平台\n",
      "《新 VR 快打專案》製作人台灣獨家專訪 打造讓老粉絲與新玩家都覺得超厲害的全新作品\n",
      " 40 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282736\n",
      "3 -- 多平台\n",
      "經典電影改編好評動作遊戲《印第安納瓊斯：古老之圈》PS5 版確定 4/17 推出\n",
      " 4 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282750\n",
      "4 -- PC\n",
      "前 Blizzard 創辦人暨總裁 Mike Morhaime 的新公司 Dreamhaven 將於 26 日凌晨公開新作\n",
      " 1 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282746\n",
      "Getting categorical news: 動漫畫\n",
      "https://gnn.gamer.com.tw/index.php?k=5\n",
      "1 -- 多平台\n",
      "網石公開全新開放世界收集型 RPG《七大罪：起源》官方預告網站\n",
      " 9 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282748\n",
      "2 -- 動漫\n",
      "新作劇場版《魔法少女小圓 瓦爾普吉斯之迴天》釋出首波視覺圖！\n",
      " 4 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282765\n",
      "3 -- 手機\n",
      "《【我推的孩子】》官方益智類手機遊戲公開前導網站 確定於全球推出\n",
      " 4 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282739\n",
      "4 -- 其他\n",
      "「吉伊卡哇燒」常設店鋪 4/7 起橫濱開幕 推出可愛甜點與原創周邊商品\n",
      " 2 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282738\n",
      "Getting categorical news: 電競\n",
      "https://gnn.gamer.com.tw/index.php?k=13\n",
      "1 -- OLG\n",
      "《英雄聯盟：奧術》第二季歌曲《Ma Meilleure Ennemie》公開 MV　Riot 公布季中邀請賽地點\n",
      " 68 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282547\n",
      "2 -- OLG\n",
      "《戰意》推出最新韓國主題「新羅」賽季　2025 CBL 即日起展開報名\n",
      " 0 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282593\n",
      "3 -- 多平台\n",
      "2025 年《跑車浪漫旅》全球系列賽將於 4 月 2 日展開線上資格賽\n",
      " 10 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282621\n",
      "4 -- Android\n",
      "ONE-NETBOOK 發表雙螢幕 Android 遊戲機「SUGAR 1」 搭載 Snapdragon G3 Gen 3 晶片\n",
      " 36 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282379\n",
      "Getting categorical news: 活動展覽\n",
      "https://gnn.gamer.com.tw/index.php?k=11\n",
      "1 -- 手機\n",
      "《明日方舟》公開韓國首場音樂會「The Symphony Of Tomorrow」周邊活動資訊\n",
      " 2 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282742\n",
      "2 -- 其他\n",
      "羚邦 x BTUHOBBYBASE 前進台中國際動漫節 周邊商品與福袋情報公開\n",
      " 0 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282771\n",
      "3 -- 其他\n",
      "《魔物獵人》公開與大阪萬博吉祥物「脈脈」聯名的一系列商品\n",
      " 40 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282723\n",
      "4 -- 其他\n",
      "《排球少年》《死神》主題曲演唱團體 SPYAIR 再度來台 6 月 Legacy TERA 熱鬧開唱\n",
      " 11 人推！\n",
      "https://gnn.gamer.com.tw/detail.php?sn=282689\n"
     ]
    }
   ],
   "source": [
    "for i, url_short_name in enumerate(news_links):\n",
    "    category = news_categories[i]\n",
    "    category_url = base_url + url_short_name\n",
    "    print(\"Getting categorical news:\", category)\n",
    "    print(category_url)\n",
    "\n",
    "    # Request the page\n",
    "    req = requests.get(category_url, headers={\"User-Agent\": user_agent.random}, timeout=5)\n",
    "    page = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    # 抓取新聞區塊\n",
    "    news_items = page.find_all('div', class_='GN-lbox2B')\n",
    "\n",
    "    serial_no = 1  # 用來計數新聞數量\n",
    "    for item in news_items:\n",
    "        # 抓取標題\n",
    "        title_tag = item.find('h1', class_='GN-lbox2D')\n",
    "        title = title_tag.text.strip() if title_tag else \"無標題\"\n",
    "\n",
    "        # 抓取新聞連結\n",
    "        link_tag = item.find('a', href=True)\n",
    "        link = urljoin(base_url, link_tag['href']) if link_tag else \"無連結\"\n",
    "\n",
    "        print(serial_no, '--', title)\n",
    "        print(link)\n",
    "\n",
    "        # 加入資料\n",
    "        categories.append(category)\n",
    "        titles.append(title)\n",
    "        links.append(link)\n",
    "\n",
    "        # 爬取內頁\n",
    "        req = requests.get(link, headers={\"User-Agent\": user_agent.random}, timeout=5)\n",
    "        page = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "        # 抓取新聞時間\n",
    "        time_element = page.find('span', class_='GN-lbox3C')\n",
    "\n",
    "        if time_element:\n",
    "            text_content = time_element.text.strip()\n",
    "            parts = text_content.split()\n",
    "            try:\n",
    "                news_time = datetime.strptime(parts[-2] + \" \" + parts[-1], \"%Y-%m-%d %H:%M:%S\")\n",
    "                news_date = news_time.strftime(\"%Y-%m-%d\")\n",
    "            except Exception as e:\n",
    "                print(f\"日期格式錯誤: {e}\")\n",
    "                news_date = \"未知\"\n",
    "        else:\n",
    "            news_date = \"未知\"\n",
    "\n",
    "        dates.append(news_date)\n",
    "\n",
    "        # 產生唯一 ID\n",
    "        item_id.append(url_short_name + \"_\" + news_date + \"_\" + str(serial_no))\n",
    "\n",
    "        # 抓取內容\n",
    "        filtered_news = [p.get_text(strip=True) for p in page.select(\".GN-lbox3B\") if p.get_text(strip=True)]\n",
    "        contents.append(filtered_news)\n",
    "\n",
    "        # **修正這裡的條件**\n",
    "        if serial_no >= 4:  # 用 `serial_no` 來限制數量，而不是 `item`\n",
    "            break\n",
    "\n",
    "        serial_no += 1  # 記得遞增計數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(item_id, dates, categories, titles, contents, links)\n",
    "df = pd.DataFrame(list(data), columns=['item_id','date','category','title','content','link'])\n",
    "df.head(2)\n",
    "df.shape\n",
    "df.content[0]\n",
    "df.to_csv(\"cna_category_news.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize news and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 16/16 [00:00<00:00, 530.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "Tokenization: 100%|██████████| 16/16 [00:00<00:00, 388.29it/s]\n",
      "Inference: 100%|██████████| 4/4 [00:20<00:00,  5.13s/it]\n",
      "Tokenization: 100%|██████████| 16/16 [00:00<00:00, 386.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize OK!\n",
      "CPU times: total: 2min 48s\n",
      "Wall time: 26.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "df = pd.read_csv('cna_category_news.csv', sep='|')\n",
    "\n",
    "# ckiplab word segment (中研院斷詞)\n",
    "# Initialize drivers\n",
    "# It takes time to download ckiplab models\n",
    "\n",
    "# default參數是model=\"bert-base\"\n",
    "# ws = CkipWordSegmenter() \n",
    "# pos = CkipPosTagger()\n",
    "# ner = CkipNerChunker()\n",
    "\n",
    "# model=\"albert-tiny\" 模型小，斷詞速度比較快，犧牲一些精確度\n",
    "ws = CkipWordSegmenter(model=\"albert-tiny\") \n",
    "pos = CkipPosTagger(model=\"albert-tiny\")\n",
    "ner = CkipNerChunker(model=\"albert-tiny\")\n",
    "\n",
    "\n",
    "## Word Segmentation\n",
    "tokens = ws(df.content)\n",
    "\n",
    "## POS\n",
    "tokens_pos = pos(tokens)\n",
    "\n",
    "## word pos pair 詞性關鍵字\n",
    "word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]\n",
    "\n",
    "## NER命名實體辨識\n",
    "entity_list = ner(df.content)\n",
    "\n",
    "# Remove stop words and filter using POS tag (tokens_v2)\n",
    "#with open('stops_chinese_traditional.txt', 'r', encoding='utf8') as f:\n",
    "#    stops = f.read().split('\\n')\n",
    "\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "# allowPOS 過濾條件: 特定的詞性\n",
    "allowPOS = ['Na', 'Nb', 'Nc', 'VC']\n",
    "\n",
    "tokens_v2 = []\n",
    "for wp in word_pos_pair:\n",
    "    tokens_v2.append([w for w, p in wp if (len(w) >= 2) and p in allowPOS])\n",
    "\n",
    "# Insert tokens into dataframe (新增斷詞資料欄位)\n",
    "df['tokens'] = tokens\n",
    "df['tokens_v2'] = tokens_v2\n",
    "df['entities'] = entity_list\n",
    "df['token_pos'] = word_pos_pair\n",
    "\n",
    "# Calculate word count (frequency) 計算字頻(次數)\n",
    "\n",
    "\n",
    "def word_frequency(wp_pair):\n",
    "    filtered_words = []\n",
    "    for word, pos in wp_pair:\n",
    "        if (pos in allowPOS) & (len(word) >= 2):\n",
    "            filtered_words.append(word)\n",
    "        #print('%s %s' % (word, pos))\n",
    "    counter = Counter(filtered_words)\n",
    "    return counter.most_common(200)\n",
    "\n",
    "\n",
    "keyfreqs = []\n",
    "for wp in word_pos_pair:\n",
    "    topwords = word_frequency(wp)\n",
    "    keyfreqs.append(topwords)\n",
    "\n",
    "df['top_key_freq'] = keyfreqs\n",
    "\n",
    "# Abstract (summary) and sentimental score(摘要與情緒分數)\n",
    "summary = []\n",
    "sentiment = []\n",
    "for text in df.content:\n",
    "    summary.append(\"暫無\")\n",
    "    sentiment.append(\"暫無\")\n",
    "\n",
    "df['summary'] = summary\n",
    "df['sentiment'] = sentiment\n",
    "\n",
    "# Rearrange the colmun order for readability\n",
    "df = df[[\n",
    "    'item_id', 'date','category', 'title', 'content', 'sentiment', 'summary',\n",
    "    'top_key_freq', 'tokens', 'tokens_v2', 'entities', 'token_pos', 'link',\n",
    "    \n",
    "]]\n",
    "\n",
    "# Save data to disk\n",
    "df.to_csv('cna_news_preprocessed.csv', sep='|', index=False)\n",
    "\n",
    "## Read it out 讀出看看\n",
    "#df = pd.read_csv('cna_dataset_preprocessed.csv', sep='|')\n",
    "#df.head(1)\n",
    "\n",
    "print(\"Tokenize OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "df = pd.read_csv('cna_news_preprocessed.csv',sep='|')\n",
    "news_categories=['PC','動漫畫','電競','活動展覽']\n",
    "# Filter condition: two words and specified POS\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "allowedPOS=['Na','Nb','Nc']\n",
    "\n",
    "# \n",
    "# get topk keyword function\n",
    "def get_top_words():\n",
    "    top_cate_words={} # final result\n",
    "    counter_all = Counter() # counter for category '全部'\n",
    "    for category in news_categories:\n",
    "\n",
    "        df_group = df[df.category == category]\n",
    "\n",
    "        # concatenate all filtered words in the same category\n",
    "        words_group = []\n",
    "        for row in df_group.token_pos:\n",
    "\n",
    "            # filter words for each news\n",
    "            filtered_words =[]\n",
    "            for (word, pos) in eval(row):\n",
    "                if (len(word) >= 2) & (pos in allowedPOS):\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "            # concatenate filtered words  \n",
    "            words_group += filtered_words\n",
    "\n",
    "        # now we can count word frequency\n",
    "        counter = Counter( words_group )\n",
    "\n",
    "        # counter \n",
    "        counter_all += counter\n",
    "        topwords = counter.most_common(100)\n",
    "\n",
    "        # store topwords\n",
    "        top_cate_words[category]= topwords\n",
    "\n",
    "    # Process category '全部'\n",
    "    top_cate_words['全部'] = counter_all.most_common(100)\n",
    "    \n",
    "    # To conveniently save data using pandas, we should convert dict to list.\n",
    "    return list(top_cate_words.items())\n",
    "\n",
    "# Save top 200 word frequency for each category\n",
    "top_group_words = get_top_words()\n",
    "df_top_group_words = pd.DataFrame(top_group_words, columns = ['category','top_keys'])\n",
    "df_top_group_words.to_csv('cna_news_topkey_with_category_via_token_pos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
