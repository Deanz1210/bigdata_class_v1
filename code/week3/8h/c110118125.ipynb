{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓三個種類新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links =['index.php?k=1', 'index.php?k=5', 'index.php?k=13','index.php?k=11']\n",
    "news_categories=['PC','動漫畫','電競','活動展覽']\n",
    "base_url = 'https://gnn.gamer.com.tw/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:135.0) Gecko/20100101 Firefox/135.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent = UserAgent()\n",
    "user_agent.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存放資料之變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "dates = []\n",
    "contents = []\n",
    "categories = []\n",
    "item_id = []\n",
    "photo_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting categorical news: PC\n",
      "https://gnn.gamer.com.tw/index.php?k=1\n",
      "1 -- 多平台\n",
      "《新 VR 快打專案》製作人台灣獨家專訪 打造讓老粉絲與新玩家都覺得超厲害的全新作品\n",
      " 37 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282736\n",
      "2 -- 多平台\n",
      "網石公開全新開放世界收集型 RPG《七大罪：起源》官方預告網站\n",
      " 0 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282748\n",
      "3 -- PC\n",
      "前 Blizzard 創辦人暨總裁 Mike Morhaime 的新公司 Dreamhaven 將於 26 日凌晨公開新作\n",
      " 1 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282746\n",
      "4 -- 多平台\n",
      "《刺客教條：暗影者》上市兩天玩家數突破 200 萬 超越《起源》與《奧德賽》首發表現\n",
      " 68 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282699\n",
      "Getting categorical news: 動漫畫\n",
      "https://gnn.gamer.com.tw/index.php?k=5\n",
      "1 -- 多平台\n",
      "網石公開全新開放世界收集型 RPG《七大罪：起源》官方預告網站\n",
      " 0 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282748\n",
      "2 -- 手機\n",
      "《【我推的孩子】》官方益智類手機遊戲公開前導網站 確定於全球推出\n",
      " 1 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282739\n",
      "3 -- 其他\n",
      "「吉伊卡哇燒」常設店鋪 4/7 起橫濱開幕 推出可愛甜點與原創周邊商品\n",
      " 1 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282738\n",
      "4 -- 動漫\n",
      "動畫《厄里斯的聖杯》公開前導視覺圖與首波宣傳影片\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282749\n",
      "Getting categorical news: 電競\n",
      "https://gnn.gamer.com.tw/index.php?k=13\n",
      "1 -- OLG\n",
      "《英雄聯盟：奧術》第二季歌曲《Ma Meilleure Ennemie》公開 MV　Riot 公布季中邀請賽地點\n",
      " 68 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282547\n",
      "2 -- OLG\n",
      "《戰意》推出最新韓國主題「新羅」賽季　2025 CBL 即日起展開報名\n",
      " 0 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282593\n",
      "3 -- 多平台\n",
      "2025 年《跑車浪漫旅》全球系列賽將於 4 月 2 日展開線上資格賽\n",
      " 10 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282621\n",
      "4 -- Android\n",
      "ONE-NETBOOK 發表雙螢幕 Android 遊戲機「SUGAR 1」 搭載 Snapdragon G3 Gen 3 晶片\n",
      " 36 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282379\n",
      "Getting categorical news: 活動展覽\n",
      "https://gnn.gamer.com.tw/index.php?k=11\n",
      "1 -- 手機\n",
      "《明日方舟》公開韓國首場音樂會「The Symphony Of Tomorrow」周邊活動資訊\n",
      " 0 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282742\n",
      "2 -- 其他\n",
      "《魔物獵人》公開與大阪萬博吉祥物「脈脈」聯名的一系列商品\n",
      " 39 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282723\n",
      "3 -- 其他\n",
      "《排球少年》《死神》主題曲演唱團體 SPYAIR 再度來台 6 月 Legacy TERA 熱鬧開唱\n",
      " 11 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282689\n",
      "4 -- 多平台\n",
      "以新登場魔物為藍本！一窺 CAPCOM CAFE《魔物獵人 荒野》合作餐點內容\n",
      " 17 人推！\n",
      "https://gnn.gamer.com.tw///gnn.gamer.com.tw/detail.php?sn=282687\n"
     ]
    }
   ],
   "source": [
    "for i, url_short_name in enumerate(news_links):\n",
    "    category = news_categories[i]\n",
    "    category_url = base_url + url_short_name\n",
    "    print(\"Getting categorical news:\", category)\n",
    "    print(category_url)\n",
    "\n",
    "    # Request the page\n",
    "    req = requests.get(category_url, headers={\"User-Agent\": user_agent.random}, timeout=5)\n",
    "    page = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    # 抓取新聞區塊\n",
    "    news_items = page.find_all('div', class_='GN-lbox2B')\n",
    "\n",
    "    serial_no = 1  # 用來計數新聞數量\n",
    "    for item in news_items:\n",
    "        # 抓取標題\n",
    "        title_tag = item.find('h1', class_='GN-lbox2D')\n",
    "        title = title_tag.text.strip() if title_tag else \"無標題\"\n",
    "\n",
    "        # 抓取新聞連結\n",
    "        link_tag = item.find('a', href=True)\n",
    "        link = base_url + link_tag['href'] if link_tag else \"無連結\"\n",
    "\n",
    "        print(serial_no, '--', title)\n",
    "        print(link)\n",
    "\n",
    "        # 加入資料\n",
    "        categories.append(category)\n",
    "        titles.append(title)\n",
    "        links.append(link)\n",
    "\n",
    "        # 爬取內頁\n",
    "        req = requests.get(link, headers={\"User-Agent\": user_agent.random}, timeout=5)\n",
    "        page = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "        # 抓取新聞時間\n",
    "        time_element = page.find('span', class_='GN-lbox3C')\n",
    "\n",
    "        if time_element:\n",
    "            text_content = time_element.text.strip()\n",
    "            parts = text_content.split()\n",
    "            try:\n",
    "                news_time = datetime.strptime(parts[-2] + \" \" + parts[-1], \"%Y-%m-%d %H:%M:%S\")\n",
    "                news_date = news_time.strftime(\"%Y-%m-%d\")\n",
    "            except Exception as e:\n",
    "                print(f\"日期格式錯誤: {e}\")\n",
    "                news_date = \"未知\"\n",
    "        else:\n",
    "            news_date = \"未知\"\n",
    "\n",
    "        dates.append(news_date)\n",
    "\n",
    "        # 產生唯一 ID\n",
    "        item_id.append(url_short_name + \"_\" + news_date + \"_\" + str(serial_no))\n",
    "\n",
    "        # 抓取內容\n",
    "        filtered_news = [p.get_text(strip=True) for p in page.select(\".GN-lbox3B\") if p.get_text(strip=True)]\n",
    "        contents.append(filtered_news)\n",
    "\n",
    "        # **修正這裡的條件**\n",
    "        if serial_no >= 4:  # 用 `serial_no` 來限制數量，而不是 `item`\n",
    "            break\n",
    "\n",
    "        serial_no += 1  # 記得遞增計數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(item_id, dates, categories, titles, contents, links)\n",
    "df = pd.DataFrame(list(data), columns=['item_id','date','category','title','content','link'])\n",
    "df.head(2)\n",
    "df.shape\n",
    "df.content[0]\n",
    "df.to_csv(\"cna_category_news.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize news and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dean4\\.cache\\huggingface\\hub\\models--ckiplab--albert-tiny-chinese-ws. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dean4\\.cache\\huggingface\\hub\\models--ckiplab--albert-tiny-chinese-pos. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dean4\\.cache\\huggingface\\hub\\models--ckiplab--albert-tiny-chinese-ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Tokenization: 100%|██████████| 16/16 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 71.22it/s]\n",
      "Tokenization: 100%|██████████| 16/16 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.47it/s]\n",
      "Tokenization: 100%|██████████| 16/16 [00:00<00:00, 15902.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 69.37it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['photo_link'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:84\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['photo_link'] not in index\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "df = pd.read_csv('cna_category_news.csv', sep='|')\n",
    "\n",
    "# ckiplab word segment (中研院斷詞)\n",
    "# Initialize drivers\n",
    "# It takes time to download ckiplab models\n",
    "\n",
    "# default參數是model=\"bert-base\"\n",
    "# ws = CkipWordSegmenter() \n",
    "# pos = CkipPosTagger()\n",
    "# ner = CkipNerChunker()\n",
    "\n",
    "# model=\"albert-tiny\" 模型小，斷詞速度比較快，犧牲一些精確度\n",
    "ws = CkipWordSegmenter(model=\"albert-tiny\") \n",
    "pos = CkipPosTagger(model=\"albert-tiny\")\n",
    "ner = CkipNerChunker(model=\"albert-tiny\")\n",
    "\n",
    "\n",
    "## Word Segmentation\n",
    "tokens = ws(df.content)\n",
    "\n",
    "## POS\n",
    "tokens_pos = pos(tokens)\n",
    "\n",
    "## word pos pair 詞性關鍵字\n",
    "word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]\n",
    "\n",
    "## NER命名實體辨識\n",
    "entity_list = ner(df.content)\n",
    "\n",
    "# Remove stop words and filter using POS tag (tokens_v2)\n",
    "#with open('stops_chinese_traditional.txt', 'r', encoding='utf8') as f:\n",
    "#    stops = f.read().split('\\n')\n",
    "\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "# allowPOS 過濾條件: 特定的詞性\n",
    "allowPOS = ['Na', 'Nb', 'Nc', 'VC']\n",
    "\n",
    "tokens_v2 = []\n",
    "for wp in word_pos_pair:\n",
    "    tokens_v2.append([w for w, p in wp if (len(w) >= 2) and p in allowPOS])\n",
    "\n",
    "# Insert tokens into dataframe (新增斷詞資料欄位)\n",
    "df['tokens'] = tokens\n",
    "df['tokens_v2'] = tokens_v2\n",
    "df['entities'] = entity_list\n",
    "df['token_pos'] = word_pos_pair\n",
    "\n",
    "# Calculate word count (frequency) 計算字頻(次數)\n",
    "\n",
    "\n",
    "def word_frequency(wp_pair):\n",
    "    filtered_words = []\n",
    "    for word, pos in wp_pair:\n",
    "        if (pos in allowPOS) & (len(word) >= 2):\n",
    "            filtered_words.append(word)\n",
    "        #print('%s %s' % (word, pos))\n",
    "    counter = Counter(filtered_words)\n",
    "    return counter.most_common(200)\n",
    "\n",
    "\n",
    "keyfreqs = []\n",
    "for wp in word_pos_pair:\n",
    "    topwords = word_frequency(wp)\n",
    "    keyfreqs.append(topwords)\n",
    "\n",
    "df['top_key_freq'] = keyfreqs\n",
    "\n",
    "# Abstract (summary) and sentimental score(摘要與情緒分數)\n",
    "summary = []\n",
    "sentiment = []\n",
    "for text in df.content:\n",
    "    summary.append(\"暫無\")\n",
    "    sentiment.append(\"暫無\")\n",
    "\n",
    "df['summary'] = summary\n",
    "df['sentiment'] = sentiment\n",
    "\n",
    "# Rearrange the colmun order for readability\n",
    "df = df[[\n",
    "    'item_id', 'date','category', 'title', 'content', 'sentiment', 'summary',\n",
    "    'top_key_freq', 'tokens', 'tokens_v2', 'entities', 'token_pos', 'link',\n",
    "    'photo_link'\n",
    "]]\n",
    "\n",
    "# Save data to disk\n",
    "df.to_csv('cna_news_preprocessed.csv', sep='|', index=False)\n",
    "\n",
    "## Read it out 讀出看看\n",
    "#df = pd.read_csv('cna_dataset_preprocessed.csv', sep='|')\n",
    "#df.head(1)\n",
    "\n",
    "print(\"Tokenize OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cna_news_preprocessed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcna_news_preprocessed.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m news_categories=[\u001b[33m'\u001b[39m\u001b[33m遊戲新聞\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m遊戲攻略\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m電競賽事\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Filter condition: two words and specified POS\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 過濾條件:兩個字以上 特定的詞性\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\ai_sch\\venv_name\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'cna_news_preprocessed.csv'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "df = pd.read_csv('cna_news_preprocessed.csv',sep='|')\n",
    "news_categories=['遊戲新聞','遊戲攻略','電競賽事']\n",
    "# Filter condition: two words and specified POS\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "allowedPOS=['Na','Nb','Nc']\n",
    "\n",
    "# \n",
    "# get topk keyword function\n",
    "def get_top_words():\n",
    "    top_cate_words={} # final result\n",
    "    counter_all = Counter() # counter for category '全部'\n",
    "    for category in news_categories:\n",
    "\n",
    "        df_group = df[df.category == category]\n",
    "\n",
    "        # concatenate all filtered words in the same category\n",
    "        words_group = []\n",
    "        for row in df_group.token_pos:\n",
    "\n",
    "            # filter words for each news\n",
    "            filtered_words =[]\n",
    "            for (word, pos) in eval(row):\n",
    "                if (len(word) >= 2) & (pos in allowedPOS):\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "            # concatenate filtered words  \n",
    "            words_group += filtered_words\n",
    "\n",
    "        # now we can count word frequency\n",
    "        counter = Counter( words_group )\n",
    "\n",
    "        # counter \n",
    "        counter_all += counter\n",
    "        topwords = counter.most_common(100)\n",
    "\n",
    "        # store topwords\n",
    "        top_cate_words[category]= topwords\n",
    "\n",
    "    # Process category '全部'\n",
    "    top_cate_words['全部'] = counter_all.most_common(100)\n",
    "    \n",
    "    # To conveniently save data using pandas, we should convert dict to list.\n",
    "    return list(top_cate_words.items())\n",
    "\n",
    "# Save top 200 word frequency for each category\n",
    "top_group_words = get_top_words()\n",
    "df_top_group_words = pd.DataFrame(top_group_words, columns = ['category','top_keys'])\n",
    "df_top_group_words.to_csv('cna_news_topkey_with_category_via_token_pos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
