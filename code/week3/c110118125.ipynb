{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓三個種類新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links =['game-news', 'game-tips', 'esports']\n",
    "news_categories=['遊戲新聞','遊戲攻略','電競賽事']\n",
    "base_url = 'https://tw.news.yahoo.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (iPhone; CPU iPhone OS 18_1_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.1.1 Mobile/15E148 Safari/604.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent = UserAgent()\n",
    "user_agent.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存放資料之變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "dates = []\n",
    "contents = []\n",
    "categories = []\n",
    "item_id = []\n",
    "photo_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting categorical news: 遊戲新聞\n",
      "https://tw.news.yahoo.com/game-news\n",
      "1 -- 《魔物獵人 荒野》玩家上市一週爆肝神速達陣「HR 999」成就！分享刷怪心得\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E9%AD%94%E7%89%A9%E7%8D%B5%E4%BA%BA-%E8%8D%92%E9%87%8E%E3%80%8B%E7%8E%A9%E5%AE%B6%E4%B8%8A%E5%B8%82%E4%B8%80%E9%80%B1%E7%88%86%E8%82%9D%E7%A5%9E%E9%80%9F%E9%81%94%E9%99%A3%E3%80%8Chr-999%E3%80%8D%E6%88%90%E5%B0%B1%EF%BC%81%E5%88%86%E4%BA%AB%E5%88%B7%E6%80%AA%E5%BF%83%E5%BE%97-081238246.html\n",
      "發布日期2025-03-08\n",
      "2 -- hololive年度大拜拜開箱！SUPER EXPO 2025、6th fes.現場照一次看\n",
      "https://tw.news.yahoo.com/hololive%E5%B9%B4%E5%BA%A6%E5%A4%A7%E6%8B%9C%E6%8B%9C%E9%96%8B%E7%AE%B1%EF%BC%81super-expo-2025%E3%80%816th-fes%E7%8F%BE%E5%A0%B4%E7%85%A7%E4%B8%80%E6%AC%A1%E7%9C%8B-071025878.html\n",
      "發布日期2025-03-08\n",
      "3 -- 《英雄聯盟》初陣對抗賽賽前訪問CFO中路超新星HongQ：「不斷進步才有好成績，希望至少四強！」【First Stand】\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%88%9D%E9%99%A3%E5%B0%8D%E6%8A%97%E8%B3%BD%E8%B3%BD%E5%89%8D%E8%A8%AA%E5%95%8Fcfo%E4%B8%AD%E8%B7%AF%E8%B6%85%E6%96%B0%E6%98%9Fhongq%EF%BC%9A%E3%80%8C%E4%B8%8D%E6%96%B7%E9%80%B2%E6%AD%A5%E6%89%8D%E6%9C%89%E5%A5%BD%E6%88%90%E7%B8%BE%EF%BC%8C%E5%B8%8C%E6%9C%9B%E8%87%B3%E5%B0%91%E5%9B%9B%E5%BC%B7%EF%BC%81%E3%80%8D%E3%80%90first-stand%E3%80%91-063915067.html\n",
      "發布日期2025-03-08\n",
      "4 -- 《英雄聯盟》初陣對抗賽賽前訪問369曝回「Karsa聖經」背後故事，更討厭換線版本：「太公式了」【First Stand】\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%88%9D%E9%99%A3%E5%B0%8D%E6%8A%97%E8%B3%BD%E8%B3%BD%E5%89%8D%E8%A8%AA%E5%95%8F369%E6%9B%9D%E5%9B%9E%E3%80%8Ckarsa%E8%81%96%E7%B6%93%E3%80%8D%E8%83%8C%E5%BE%8C%E6%95%85%E4%BA%8B%EF%BC%8C%E6%9B%B4%E8%A8%8E%E5%8E%AD%E6%8F%9B%E7%B7%9A%E7%89%88%E6%9C%AC%EF%BC%9A%E3%80%8C%E5%A4%AA%E5%85%AC%E5%BC%8F%E4%BA%86%E3%80%8D%E3%80%90first-stand%E3%80%91-050107087.html\n",
      "發布日期2025-03-08\n",
      "Getting categorical news: 遊戲攻略\n",
      "https://tw.news.yahoo.com/game-tips\n",
      "1 -- 《鳴潮》新角色「布蘭特」攻略：機制介紹、共鳴鏈分析、武器選擇、隊伍搭配、聲骸推薦\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E9%B3%B4%E6%BD%AE%E3%80%8B%E6%96%B0%E8%A7%92%E8%89%B2%E3%80%8C%E5%B8%83%E8%98%AD%E7%89%B9%E3%80%8D%E6%94%BB%E7%95%A5%EF%BC%9A%E6%A9%9F%E5%88%B6%E4%BB%8B%E7%B4%B9%E3%80%81%E5%85%B1%E9%B3%B4%E9%8F%88%E5%88%86%E6%9E%90%E3%80%81%E6%AD%A6%E5%99%A8%E9%81%B8%E6%93%87%E3%80%81%E9%9A%8A%E4%BC%8D%E6%90%AD%E9%85%8D%E3%80%81%E8%81%B2%E9%AA%B8%E6%8E%A8%E8%96%A6-091326594.html\n",
      "發布日期2025-03-07\n",
      "2 -- 端麗翼鳥、火焰傘蜥、雪球蟲、虛無咚、沙之流星地點一次看！《魔物獵人 荒野》環境生物支線任務攻略\n",
      "https://tw.news.yahoo.com/%E7%AB%AF%E9%BA%97%E7%BF%BC%E9%B3%A5%E3%80%81%E7%81%AB%E7%84%B0%E5%82%98%E8%9C%A5%E3%80%81%E9%9B%AA%E7%90%83%E8%9F%B2%E3%80%81%E8%99%9B%E7%84%A1%E5%92%9A%E3%80%81%E6%B2%99%E4%B9%8B%E6%B5%81%E6%98%9F%E5%9C%B0%E9%BB%9E%E4%B8%80%E6%AC%A1%E7%9C%8B%EF%BC%81%E3%80%8A%E9%AD%94%E7%89%A9%E7%8D%B5%E4%BA%BA-%E8%8D%92%E9%87%8E%E3%80%8B%E7%92%B0%E5%A2%83%E7%94%9F%E7%89%A9%E6%94%AF%E7%B7%9A%E4%BB%BB%E5%8B%99%E6%94%BB%E7%95%A5-062907059.html\n",
      "發布日期2025-03-07\n",
      "3 -- 《魔物獵人 荒野》超實用技巧！1塊生肉烤出12個全熟肉的方法\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E9%AD%94%E7%89%A9%E7%8D%B5%E4%BA%BA-%E8%8D%92%E9%87%8E%E3%80%8B%E8%B6%85%E5%AF%A6%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%811%E5%A1%8A%E7%94%9F%E8%82%89%E7%83%A4%E5%87%BA12%E5%80%8B%E5%85%A8%E7%86%9F%E8%82%89%E7%9A%84%E6%96%B9%E6%B3%95-065542306.html\n",
      "發布日期2025-03-03\n",
      "4 -- 《魔物獵人 荒野》次要使命「足跡蜥蜴」在哪裡？捕捉地點快速看\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E9%AD%94%E7%89%A9%E7%8D%B5%E4%BA%BA-%E8%8D%92%E9%87%8E%E3%80%8B%E6%AC%A1%E8%A6%81%E4%BD%BF%E5%91%BD%E3%80%8C%E8%B6%B3%E8%B7%A1%E8%9C%A5%E8%9C%B4%E3%80%8D%E5%9C%A8%E5%93%AA%E8%A3%A1%EF%BC%9F%E6%8D%95%E6%8D%89%E5%9C%B0%E9%BB%9E%E5%BF%AB%E9%80%9F%E7%9C%8B-144234727.html\n",
      "發布日期2025-03-01\n",
      "Getting categorical news: 電競賽事\n",
      "https://tw.news.yahoo.com/esports\n",
      "1 -- 《英雄聯盟》初陣對抗賽賽前訪問CFO中路超新星HongQ：「不斷進步才有好成績，希望至少四強！」【First Stand】\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%88%9D%E9%99%A3%E5%B0%8D%E6%8A%97%E8%B3%BD%E8%B3%BD%E5%89%8D%E8%A8%AA%E5%95%8Fcfo%E4%B8%AD%E8%B7%AF%E8%B6%85%E6%96%B0%E6%98%9Fhongq%EF%BC%9A%E3%80%8C%E4%B8%8D%E6%96%B7%E9%80%B2%E6%AD%A5%E6%89%8D%E6%9C%89%E5%A5%BD%E6%88%90%E7%B8%BE%EF%BC%8C%E5%B8%8C%E6%9C%9B%E8%87%B3%E5%B0%91%E5%9B%9B%E5%BC%B7%EF%BC%81%E3%80%8D%E3%80%90first-stand%E3%80%91-063915067.html\n",
      "發布日期2025-03-08\n",
      "2 -- 《英雄聯盟》初陣對抗賽賽前訪問369曝回「Karsa聖經」背後故事，更討厭換線版本：「太公式了」【First Stand】\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%88%9D%E9%99%A3%E5%B0%8D%E6%8A%97%E8%B3%BD%E8%B3%BD%E5%89%8D%E8%A8%AA%E5%95%8F369%E6%9B%9D%E5%9B%9E%E3%80%8Ckarsa%E8%81%96%E7%B6%93%E3%80%8D%E8%83%8C%E5%BE%8C%E6%95%85%E4%BA%8B%EF%BC%8C%E6%9B%B4%E8%A8%8E%E5%8E%AD%E6%8F%9B%E7%B7%9A%E7%89%88%E6%9C%AC%EF%BC%9A%E3%80%8C%E5%A4%AA%E5%85%AC%E5%BC%8F%E4%BA%86%E3%80%8D%E3%80%90first-stand%E3%80%91-050107087.html\n",
      "發布日期2025-03-08\n",
      "3 -- 《英雄聯盟》初陣對抗賽賽前訪問Peanut感謝台灣粉絲應援十週年，認目前版本「藍方的確有優勢」【First Stand】\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%88%9D%E9%99%A3%E5%B0%8D%E6%8A%97%E8%B3%BD%E8%B3%BD%E5%89%8D%E8%A8%AA%E5%95%8Fpeanut%E6%84%9F%E8%AC%9D%E5%8F%B0%E7%81%A3%E7%B2%89%E7%B5%B2%E6%87%89%E6%8F%B4%E5%8D%81%E9%80%B1%E5%B9%B4%EF%BC%8C%E8%AA%8D%E7%9B%AE%E5%89%8D%E7%89%88%E6%9C%AC%E3%80%8C%E8%97%8D%E6%96%B9%E7%9A%84%E7%A2%BA%E6%9C%89%E5%84%AA%E5%8B%A2%E3%80%8D%E3%80%90first-stand%E3%80%91-025805761.html\n",
      "發布日期2025-03-08\n",
      "4 -- 《快打旋風6》台灣「五股石油王」沒能打出夢幻劇本！年度總決賽小組止步\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E5%BF%AB%E6%89%93%E6%97%8B%E9%A2%A86%E3%80%8B%E5%8F%B0%E7%81%A3%E3%80%8C%E4%BA%94%E8%82%A1%E7%9F%B3%E6%B2%B9%E7%8E%8B%E3%80%8D%E6%B2%92%E8%83%BD%E6%89%93%E5%87%BA%E5%A4%A2%E5%B9%BB%E5%8A%87%E6%9C%AC%EF%BC%81%E5%B9%B4%E5%BA%A6%E7%B8%BD%E6%B1%BA%E8%B3%BD%E5%B0%8F%E7%B5%84%E6%AD%A2%E6%AD%A5-123205349.html\n",
      "發布日期2025-03-07\n"
     ]
    }
   ],
   "source": [
    "for i, url_short_name in enumerate(news_links):  #針對每一類 共有3類\n",
    "\n",
    "    category = news_categories[i]  #類別名稱紀錄起來 \n",
    "\n",
    "    # Categorical url link\n",
    "    category_url = base_url + url_short_name \n",
    "    print(\"Getting categorical news:\", category)\n",
    "    print(category_url)\n",
    "    # Request the categorical news page\n",
    "    # req = requests.get(category_url)\n",
    "    req = requests.get(category_url, headers={ 'user-agent': user_agent.random }, timeout=5)\n",
    "    page = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    # 抓新聞列表\n",
    "    items = page.find_all('li', class_='stream-item')\n",
    "    #print(items)\n",
    "\n",
    "\n",
    "    # Let's start to crawl the news in the first page for that category\n",
    "    serial_no = 1\n",
    "    for item_j, item in enumerate(items,start=1): #針對每一篇項目 抓其細節\n",
    "        title = item.find('h3').text\n",
    "        print(serial_no,'--', title )\n",
    "        \n",
    "        link = item.find('a').get('href')\n",
    "        link = \"https://tw.news.yahoo.com\"+link\n",
    "        print(link)\n",
    "\n",
    "        # 儲存圖片網址變數\n",
    "        image_url = None\n",
    "\n",
    "# **方法 1: 嘗試直接抓取 <figure> 內的 <img>**\n",
    "        figure_tag = page.find(\"figure\", class_=\"caas-figure\")\n",
    "        if figure_tag:\n",
    "            img_tag = figure_tag.find(\"img\")\n",
    "            if img_tag:\n",
    "                image_url = img_tag.get(\"src\") or img_tag.get(\"data-src\")  # 優先抓 src，若無則抓 data-src\n",
    "\n",
    "\n",
    "# **Yahoo 可能返回錯誤的圖片網址 (包含兩個 URL)，需要修正**\n",
    "        if image_url and \"https://s.yimg.com/ny/api/res/\" in image_url:\n",
    "            match = re.search(r'(https://s\\.yimg\\.com/os/creatr-uploaded-images/[\\w./-]+)', image_url)\n",
    "            if match:\n",
    "                image_url = match.group(0)\n",
    "\n",
    "        \n",
    "       \n",
    "      \n",
    "        #print(f'發布於:'+news_date)\n",
    "        categories.append(category)\n",
    "        titles.append(title)\n",
    "        links.append(link)\n",
    "        #photo_links.append(image_url)\n",
    "        \n",
    "        \n",
    "        req = requests.get(link, headers={ 'user-agent': user_agent.random }, timeout=5)\n",
    "        \n",
    "        page = BeautifulSoup(req.text,'html.parser')\n",
    "        time_element = page.find('time', {'class': \"caas-attr-meta-time\"})\n",
    "\n",
    "        if time_element:\n",
    "            news_time_iso = time_element['datetime']  # 擷取 ISO 8601 格式，如：\"2025-02-26T07:12:09.000Z\"\n",
    "\n",
    "            news_time = datetime.strptime(news_time_iso, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "    # 轉換格式\n",
    "            dtime = news_time.strftime(\"%Y/%m/%d %H:%M\")  # 轉換為 \"YYYY/MM/DD HH:MM\" 格式\n",
    "            news_date = news_time.strftime(\"%Y-%m-%d\")  # 轉換為 \"YYYY-MM-DD\" 格式\n",
    "        else:\n",
    "            dtime = \"未知\"\n",
    "            news_date = \"未知\"\n",
    "        print(f\"發布日期\"+news_date)\n",
    "        dates.append(news_date)\n",
    "        \n",
    "        # 儲存圖片網址變數\n",
    "        image_url = None\n",
    "\n",
    "# **方法 1: 嘗試直接抓取 <figure> 內的 <img>**\n",
    "        figure_tag = page.find(\"figure\", class_=\"caas-figure\")\n",
    "        if figure_tag:\n",
    "            img_tag = figure_tag.find(\"img\")\n",
    "            if img_tag:\n",
    "                image_url = img_tag.get(\"src\") or img_tag.get(\"data-src\")  # 優先抓 src，若無則抓 data-src\n",
    "\n",
    "\n",
    "# **Yahoo 可能返回錯誤的圖片網址 (包含兩個 URL)，需要修正**\n",
    "        if image_url and \"https://s.yimg.com/ny/api/res/\" in image_url:\n",
    "            match = re.search(r'(https://s\\.yimg\\.com/os/creatr-uploaded-images/[\\w./-]+)', image_url)\n",
    "            if match:\n",
    "                image_url = match.group(0)\n",
    "        photo_links.append(image_url)\n",
    "                \n",
    "        item_id.append(url_short_name + \"_\" + news_date + \"_\" + str(serial_no))\n",
    "        serial_no += 1\n",
    "        # Find content\n",
    "        content = [p.text.strip() for p in page.select(\".caas-body p\")]\n",
    "        filtered_news = [item for item in content if not item.startswith(\"相關新聞\")]\n",
    "       \n",
    "        contents.append(filtered_news)\n",
    "        \n",
    "        if item_j >= 4: # Here we crawl only 4 pieces of news for each category, in order to save time.\n",
    "            break \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(item_id, dates, categories, titles, contents, links, photo_links)\n",
    "df = pd.DataFrame(list(data), columns=['item_id','date','category','title','content','link','photo_link'])\n",
    "df.head(2)\n",
    "df.shape\n",
    "df.content[0]\n",
    "df.to_csv(\"cna_category_news.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize news and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project_code\\bigdata_class_v1\\venv_bigdata\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Tokenization: 100%|██████████| 12/12 [00:00<00:00, 1300.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Tokenization: 100%|██████████| 12/12 [00:00<00:00, 1410.05it/s]\n",
      "Inference: 100%|██████████| 3/3 [00:11<00:00,  3.80s/it]\n",
      "Tokenization: 100%|██████████| 12/12 [00:00<00:00, 1197.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize OK!\n",
      "CPU times: total: 1min 4s\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "df = pd.read_csv('cna_category_news.csv', sep='|')\n",
    "\n",
    "# ckiplab word segment (中研院斷詞)\n",
    "# Initialize drivers\n",
    "# It takes time to download ckiplab models\n",
    "\n",
    "# default參數是model=\"bert-base\"\n",
    "# ws = CkipWordSegmenter() \n",
    "# pos = CkipPosTagger()\n",
    "# ner = CkipNerChunker()\n",
    "\n",
    "# model=\"albert-tiny\" 模型小，斷詞速度比較快，犧牲一些精確度\n",
    "ws = CkipWordSegmenter(model=\"albert-tiny\") \n",
    "pos = CkipPosTagger(model=\"albert-tiny\")\n",
    "ner = CkipNerChunker(model=\"albert-tiny\")\n",
    "\n",
    "\n",
    "## Word Segmentation\n",
    "tokens = ws(df.content)\n",
    "\n",
    "## POS\n",
    "tokens_pos = pos(tokens)\n",
    "\n",
    "## word pos pair 詞性關鍵字\n",
    "word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]\n",
    "\n",
    "## NER命名實體辨識\n",
    "entity_list = ner(df.content)\n",
    "\n",
    "# Remove stop words and filter using POS tag (tokens_v2)\n",
    "#with open('stops_chinese_traditional.txt', 'r', encoding='utf8') as f:\n",
    "#    stops = f.read().split('\\n')\n",
    "\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "# allowPOS 過濾條件: 特定的詞性\n",
    "allowPOS = ['Na', 'Nb', 'Nc', 'VC']\n",
    "\n",
    "tokens_v2 = []\n",
    "for wp in word_pos_pair:\n",
    "    tokens_v2.append([w for w, p in wp if (len(w) >= 2) and p in allowPOS])\n",
    "\n",
    "# Insert tokens into dataframe (新增斷詞資料欄位)\n",
    "df['tokens'] = tokens\n",
    "df['tokens_v2'] = tokens_v2\n",
    "df['entities'] = entity_list\n",
    "df['token_pos'] = word_pos_pair\n",
    "\n",
    "# Calculate word count (frequency) 計算字頻(次數)\n",
    "\n",
    "\n",
    "def word_frequency(wp_pair):\n",
    "    filtered_words = []\n",
    "    for word, pos in wp_pair:\n",
    "        if (pos in allowPOS) & (len(word) >= 2):\n",
    "            filtered_words.append(word)\n",
    "        #print('%s %s' % (word, pos))\n",
    "    counter = Counter(filtered_words)\n",
    "    return counter.most_common(200)\n",
    "\n",
    "\n",
    "keyfreqs = []\n",
    "for wp in word_pos_pair:\n",
    "    topwords = word_frequency(wp)\n",
    "    keyfreqs.append(topwords)\n",
    "\n",
    "df['top_key_freq'] = keyfreqs\n",
    "\n",
    "# Abstract (summary) and sentimental score(摘要與情緒分數)\n",
    "summary = []\n",
    "sentiment = []\n",
    "for text in df.content:\n",
    "    summary.append(\"暫無\")\n",
    "    sentiment.append(\"暫無\")\n",
    "\n",
    "df['summary'] = summary\n",
    "df['sentiment'] = sentiment\n",
    "\n",
    "# Rearrange the colmun order for readability\n",
    "df = df[[\n",
    "    'item_id', 'date','category', 'title', 'content', 'sentiment', 'summary',\n",
    "    'top_key_freq', 'tokens', 'tokens_v2', 'entities', 'token_pos', 'link',\n",
    "    'photo_link'\n",
    "]]\n",
    "\n",
    "# Save data to disk\n",
    "df.to_csv('cna_news_preprocessed.csv', sep='|', index=False)\n",
    "\n",
    "## Read it out 讀出看看\n",
    "#df = pd.read_csv('cna_dataset_preprocessed.csv', sep='|')\n",
    "#df.head(1)\n",
    "\n",
    "print(\"Tokenize OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "df = pd.read_csv('cna_news_preprocessed.csv',sep='|')\n",
    "news_categories=['遊戲新聞','遊戲攻略','電競賽事']\n",
    "# Filter condition: two words and specified POS\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "allowedPOS=['Na','Nb','Nc']\n",
    "\n",
    "# \n",
    "# get topk keyword function\n",
    "def get_top_words():\n",
    "    top_cate_words={} # final result\n",
    "    counter_all = Counter() # counter for category '全部'\n",
    "    for category in news_categories:\n",
    "\n",
    "        df_group = df[df.category == category]\n",
    "\n",
    "        # concatenate all filtered words in the same category\n",
    "        words_group = []\n",
    "        for row in df_group.token_pos:\n",
    "\n",
    "            # filter words for each news\n",
    "            filtered_words =[]\n",
    "            for (word, pos) in eval(row):\n",
    "                if (len(word) >= 2) & (pos in allowedPOS):\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "            # concatenate filtered words  \n",
    "            words_group += filtered_words\n",
    "\n",
    "        # now we can count word frequency\n",
    "        counter = Counter( words_group )\n",
    "\n",
    "        # counter \n",
    "        counter_all += counter\n",
    "        topwords = counter.most_common(100)\n",
    "\n",
    "        # store topwords\n",
    "        top_cate_words[category]= topwords\n",
    "\n",
    "    # Process category '全部'\n",
    "    top_cate_words['全部'] = counter_all.most_common(100)\n",
    "    \n",
    "    # To conveniently save data using pandas, we should convert dict to list.\n",
    "    return list(top_cate_words.items())\n",
    "\n",
    "# Save top 200 word frequency for each category\n",
    "top_group_words = get_top_words()\n",
    "df_top_group_words = pd.DataFrame(top_group_words, columns = ['category','top_keys'])\n",
    "df_top_group_words.to_csv('cna_news_topkey_with_category_via_token_pos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
