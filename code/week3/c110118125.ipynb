{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓三個種類新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links =['game-news', 'game-tips', 'esports']\n",
    "news_categories=['遊戲新聞','遊戲攻略','電競賽事']\n",
    "base_url = 'https://tw.news.yahoo.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent = UserAgent()\n",
    "user_agent.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存放資料之變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "dates = []\n",
    "contents = []\n",
    "categories = []\n",
    "item_id = []\n",
    "photo_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting categorical news: 遊戲新聞\n",
      "https://tw.news.yahoo.com/game-news\n",
      "1 -- 《戰神》正式邁入弒神之旅20週年，官方免費推遊戲內套裝、桌布供粉絲下載，還有紀念T等週邊讓玩家入手\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E6%88%B0%E7%A5%9E%E3%80%8B%E6%AD%A3%E5%BC%8F%E9%82%81%E5%85%A5%E5%BC%92%E7%A5%9E%E4%B9%8B%E6%97%8520%E9%80%B1%E5%B9%B4%EF%BC%8C%E5%AE%98%E6%96%B9%E5%85%8D%E8%B2%BB%E6%8E%A8%E9%81%8A%E6%88%B2%E5%85%A7%E5%A5%97%E8%A3%9D%E3%80%81%E6%A1%8C%E5%B8%83%E4%BE%9B%E7%B2%89%E7%B5%B2%E4%B8%8B%E8%BC%89%EF%BC%8C%E9%82%84%E6%9C%89%E7%B4%80%E5%BF%B5t%E7%AD%89%E9%80%B1%E9%82%8A%E8%AE%93%E7%8E%A9%E5%AE%B6%E5%85%A5%E6%89%8B-031018902.html\n",
      "發布日期2025-03-22\n",
      "2 -- 《鳴潮》宣布4月29日登上Steam！2.2版前瞻兌換碼公開\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E9%B3%B4%E6%BD%AE%E3%80%8B%E5%AE%A3%E5%B8%834%E6%9C%8829%E6%97%A5%E7%99%BB%E4%B8%8Asteam%EF%BC%8122%E7%89%88%E5%89%8D%E7%9E%BB%E5%85%8C%E6%8F%9B%E7%A2%BC%E5%85%AC%E9%96%8B-192145445.html\n",
      "發布日期2025-03-21\n",
      "3 -- 《Pokemon TCG Pocket》新卡包「嗨放異彩」3月27日上線！收錄作品曝光，異色寶可夢首次登場\n",
      "https://tw.news.yahoo.com/%E3%80%8Apokemon-tcg-pocket%E3%80%8B%E6%96%B0%E5%8D%A1%E5%8C%85%E3%80%8C%E5%97%A8%E6%94%BE%E7%95%B0%E5%BD%A9%E3%80%8D3%E6%9C%8827%E6%97%A5%E4%B8%8A%E7%B7%9A%EF%BC%81%E6%94%B6%E9%8C%84%E4%BD%9C%E5%93%81%E6%9B%9D%E5%85%89%EF%BC%8C%E7%95%B0%E8%89%B2%E5%AF%B6%E5%8F%AF%E5%A4%A2%E9%A6%96%E6%AC%A1%E7%99%BB%E5%A0%B4-185413692.html\n",
      "發布日期2025-03-21\n",
      "4 -- 《英雄聯盟》Faker冠軍造型不是加里歐？被爆料選了「犽凝」\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8Bfaker%E5%86%A0%E8%BB%8D%E9%80%A0%E5%9E%8B%E4%B8%8D%E6%98%AF%E5%8A%A0%E9%87%8C%E6%AD%90%EF%BC%9F%E8%A2%AB%E7%88%86%E6%96%99%E9%81%B8%E4%BA%86%E3%80%8C%E7%8A%BD%E5%87%9D%E3%80%8D-180511116.html\n",
      "發布日期2025-03-21\n",
      "Getting categorical news: 遊戲攻略\n",
      "https://tw.news.yahoo.com/game-tips\n",
      "Getting categorical news: 電競賽事\n",
      "https://tw.news.yahoo.com/esports\n",
      "1 -- 《英雄聯盟》Faker冠軍造型不是加里歐？被爆料選了「犽凝」\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8Bfaker%E5%86%A0%E8%BB%8D%E9%80%A0%E5%9E%8B%E4%B8%8D%E6%98%AF%E5%8A%A0%E9%87%8C%E6%AD%90%EF%BC%9F%E8%A2%AB%E7%88%86%E6%96%99%E9%81%B8%E4%BA%86%E3%80%8C%E7%8A%BD%E5%87%9D%E3%80%8D-180511116.html\n",
      "發布日期2025-03-21\n",
      "2 -- 《英雄聯盟》T1 CEO宣布Gumayusi重回首發，引質疑管理層是否強勢介入教練決策\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8Bt1-ceo%E5%AE%A3%E5%B8%83gumayusi%E9%87%8D%E5%9B%9E%E9%A6%96%E7%99%BC%EF%BC%8C%E5%BC%95%E8%B3%AA%E7%96%91%E7%AE%A1%E7%90%86%E5%B1%A4%E6%98%AF%E5%90%A6%E5%BC%B7%E5%8B%A2%E4%BB%8B%E5%85%A5%E6%95%99%E7%B7%B4%E6%B1%BA%E7%AD%96-042556807.html\n",
      "發布日期2025-03-20\n",
      "3 -- 《英雄聯盟》初陣對抗賽TOP5操作出爐，HLE成功霸榜 但CFO Doggo也獲選\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%88%9D%E9%99%A3%E5%B0%8D%E6%8A%97%E8%B3%BDtop5%E6%93%8D%E4%BD%9C%E5%87%BA%E7%88%90%EF%BC%8Chle%E6%88%90%E5%8A%9F%E9%9C%B8%E6%A6%9C-%E4%BD%86cfo-doggo%E4%B9%9F%E7%8D%B2%E9%81%B8-031008803.html\n",
      "發布日期2025-03-20\n",
      "4 -- 《英雄聯盟》姑媽回來了！T1宣布Gumayusi新賽季先發登場\n",
      "https://tw.news.yahoo.com/%E3%80%8A%E8%8B%B1%E9%9B%84%E8%81%AF%E7%9B%9F%E3%80%8B%E5%A7%91%E5%AA%BD%E5%9B%9E%E4%BE%86%E4%BA%86%EF%BC%81t1%E5%AE%A3%E5%B8%83gumayusi%E6%96%B0%E8%B3%BD%E5%AD%A3%E5%85%88%E7%99%BC%E7%99%BB%E5%A0%B4-164500714.html\n",
      "發布日期2025-03-19\n"
     ]
    }
   ],
   "source": [
    "for i, url_short_name in enumerate(news_links):  #針對每一類 共有3類\n",
    "\n",
    "    category = news_categories[i]  #類別名稱紀錄起來 \n",
    "\n",
    "    # Categorical url link\n",
    "    category_url = base_url + url_short_name \n",
    "    print(\"Getting categorical news:\", category)\n",
    "    print(category_url)\n",
    "    # Request the categorical news page\n",
    "    # req = requests.get(category_url)\n",
    "    req = requests.get(category_url, headers={ 'user-agent': user_agent.random }, timeout=5)\n",
    "    page = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    # 抓新聞列表\n",
    "    items = page.find_all('li', class_='stream-item')\n",
    "    #print(items)\n",
    "\n",
    "\n",
    "    # Let's start to crawl the news in the first page for that category\n",
    "    serial_no = 1\n",
    "    for item_j, item in enumerate(items,start=1): #針對每一篇項目 抓其細節\n",
    "        title = item.find('h3').text\n",
    "        print(serial_no,'--', title )\n",
    "        \n",
    "        link = item.find('a').get('href')\n",
    "        link = \"https://tw.news.yahoo.com\"+link\n",
    "        print(link)\n",
    "\n",
    "        # 儲存圖片網址變數\n",
    "        image_url = None\n",
    "\n",
    "# **方法 1: 嘗試直接抓取 <figure> 內的 <img>**\n",
    "        figure_tag = page.find(\"figure\", class_=\"caas-figure\")\n",
    "        if figure_tag:\n",
    "            img_tag = figure_tag.find(\"img\")\n",
    "            if img_tag:\n",
    "                image_url = img_tag.get(\"src\") or img_tag.get(\"data-src\")  # 優先抓 src，若無則抓 data-src\n",
    "\n",
    "\n",
    "# **Yahoo 可能返回錯誤的圖片網址 (包含兩個 URL)，需要修正**\n",
    "        if image_url and \"https://s.yimg.com/ny/api/res/\" in image_url:\n",
    "            match = re.search(r'(https://s\\.yimg\\.com/os/creatr-uploaded-images/[\\w./-]+)', image_url)\n",
    "            if match:\n",
    "                image_url = match.group(0)\n",
    "\n",
    "        \n",
    "       \n",
    "      \n",
    "        #print(f'發布於:'+news_date)\n",
    "        categories.append(category)\n",
    "        titles.append(title)\n",
    "        links.append(link)\n",
    "        #photo_links.append(image_url)\n",
    "        \n",
    "        \n",
    "        req = requests.get(link, headers={ 'user-agent': user_agent.random }, timeout=5)\n",
    "        \n",
    "        page = BeautifulSoup(req.text,'html.parser')\n",
    "        time_element = page.find('time', {'class': \"caas-attr-meta-time\"})\n",
    "\n",
    "        if time_element:\n",
    "            news_time_iso = time_element['datetime']  # 擷取 ISO 8601 格式，如：\"2025-02-26T07:12:09.000Z\"\n",
    "\n",
    "            news_time = datetime.strptime(news_time_iso, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "    # 轉換格式\n",
    "            dtime = news_time.strftime(\"%Y/%m/%d %H:%M\")  # 轉換為 \"YYYY/MM/DD HH:MM\" 格式\n",
    "            news_date = news_time.strftime(\"%Y-%m-%d\")  # 轉換為 \"YYYY-MM-DD\" 格式\n",
    "        else:\n",
    "            dtime = \"未知\"\n",
    "            news_date = \"未知\"\n",
    "        print(f\"發布日期\"+news_date)\n",
    "        dates.append(news_date)\n",
    "        \n",
    "        # 儲存圖片網址變數\n",
    "        image_url = None\n",
    "\n",
    "# **方法 1: 嘗試直接抓取 <figure> 內的 <img>**\n",
    "        figure_tag = page.find(\"figure\", class_=\"caas-figure\")\n",
    "        if figure_tag:\n",
    "            img_tag = figure_tag.find(\"img\")\n",
    "            if img_tag:\n",
    "                image_url = img_tag.get(\"src\") or img_tag.get(\"data-src\")  # 優先抓 src，若無則抓 data-src\n",
    "\n",
    "\n",
    "# **Yahoo 可能返回錯誤的圖片網址 (包含兩個 URL)，需要修正**\n",
    "        if image_url and \"https://s.yimg.com/ny/api/res/\" in image_url:\n",
    "            match = re.search(r'(https://s\\.yimg\\.com/os/creatr-uploaded-images/[\\w./-]+)', image_url)\n",
    "            if match:\n",
    "                image_url = match.group(0)\n",
    "        photo_links.append(image_url)\n",
    "                \n",
    "        item_id.append(url_short_name + \"_\" + news_date + \"_\" + str(serial_no))\n",
    "        serial_no += 1\n",
    "        # Find content\n",
    "        content = [p.text.strip() for p in page.select(\".caas-body p\")]\n",
    "        filtered_news = [item for item in content if not item.startswith(\"相關新聞\")]\n",
    "       \n",
    "        contents.append(filtered_news)\n",
    "        \n",
    "        if item_j >= 4: # Here we crawl only 4 pieces of news for each category, in order to save time.\n",
    "            break \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(item_id, dates, categories, titles, contents, links, photo_links)\n",
    "df = pd.DataFrame(list(data), columns=['item_id','date','category','title','content','link','photo_link'])\n",
    "df.head(2)\n",
    "df.shape\n",
    "df.content[0]\n",
    "df.to_csv(\"cna_category_news.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize news and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project_code\\bigdata_class_v1\\venv_bigdata\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Tokenization: 100%|██████████| 8/8 [00:00<00:00, 1924.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
      "Tokenization: 100%|██████████| 8/8 [00:00<00:00, 2001.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "Tokenization: 100%|██████████| 8/8 [00:00<00:00, 2001.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize OK!\n",
      "CPU times: total: 9.62 s\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "df = pd.read_csv('cna_category_news.csv', sep='|')\n",
    "\n",
    "# ckiplab word segment (中研院斷詞)\n",
    "# Initialize drivers\n",
    "# It takes time to download ckiplab models\n",
    "\n",
    "# default參數是model=\"bert-base\"\n",
    "# ws = CkipWordSegmenter() \n",
    "# pos = CkipPosTagger()\n",
    "# ner = CkipNerChunker()\n",
    "\n",
    "# model=\"albert-tiny\" 模型小，斷詞速度比較快，犧牲一些精確度\n",
    "ws = CkipWordSegmenter(model=\"albert-tiny\") \n",
    "pos = CkipPosTagger(model=\"albert-tiny\")\n",
    "ner = CkipNerChunker(model=\"albert-tiny\")\n",
    "\n",
    "\n",
    "## Word Segmentation\n",
    "tokens = ws(df.content)\n",
    "\n",
    "## POS\n",
    "tokens_pos = pos(tokens)\n",
    "\n",
    "## word pos pair 詞性關鍵字\n",
    "word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]\n",
    "\n",
    "## NER命名實體辨識\n",
    "entity_list = ner(df.content)\n",
    "\n",
    "# Remove stop words and filter using POS tag (tokens_v2)\n",
    "#with open('stops_chinese_traditional.txt', 'r', encoding='utf8') as f:\n",
    "#    stops = f.read().split('\\n')\n",
    "\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "# allowPOS 過濾條件: 特定的詞性\n",
    "allowPOS = ['Na', 'Nb', 'Nc', 'VC']\n",
    "\n",
    "tokens_v2 = []\n",
    "for wp in word_pos_pair:\n",
    "    tokens_v2.append([w for w, p in wp if (len(w) >= 2) and p in allowPOS])\n",
    "\n",
    "# Insert tokens into dataframe (新增斷詞資料欄位)\n",
    "df['tokens'] = tokens\n",
    "df['tokens_v2'] = tokens_v2\n",
    "df['entities'] = entity_list\n",
    "df['token_pos'] = word_pos_pair\n",
    "\n",
    "# Calculate word count (frequency) 計算字頻(次數)\n",
    "\n",
    "\n",
    "def word_frequency(wp_pair):\n",
    "    filtered_words = []\n",
    "    for word, pos in wp_pair:\n",
    "        if (pos in allowPOS) & (len(word) >= 2):\n",
    "            filtered_words.append(word)\n",
    "        #print('%s %s' % (word, pos))\n",
    "    counter = Counter(filtered_words)\n",
    "    return counter.most_common(200)\n",
    "\n",
    "\n",
    "keyfreqs = []\n",
    "for wp in word_pos_pair:\n",
    "    topwords = word_frequency(wp)\n",
    "    keyfreqs.append(topwords)\n",
    "\n",
    "df['top_key_freq'] = keyfreqs\n",
    "\n",
    "# Abstract (summary) and sentimental score(摘要與情緒分數)\n",
    "summary = []\n",
    "sentiment = []\n",
    "for text in df.content:\n",
    "    summary.append(\"暫無\")\n",
    "    sentiment.append(\"暫無\")\n",
    "\n",
    "df['summary'] = summary\n",
    "df['sentiment'] = sentiment\n",
    "\n",
    "# Rearrange the colmun order for readability\n",
    "df = df[[\n",
    "    'item_id', 'date','category', 'title', 'content', 'sentiment', 'summary',\n",
    "    'top_key_freq', 'tokens', 'tokens_v2', 'entities', 'token_pos', 'link',\n",
    "    'photo_link'\n",
    "]]\n",
    "\n",
    "# Save data to disk\n",
    "df.to_csv('cna_news_preprocessed.csv', sep='|', index=False)\n",
    "\n",
    "## Read it out 讀出看看\n",
    "#df = pd.read_csv('cna_dataset_preprocessed.csv', sep='|')\n",
    "#df.head(1)\n",
    "\n",
    "print(\"Tokenize OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count top keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "df = pd.read_csv('cna_news_preprocessed.csv',sep='|')\n",
    "news_categories=['遊戲新聞','遊戲攻略','電競賽事']\n",
    "# Filter condition: two words and specified POS\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "allowedPOS=['Na','Nb','Nc']\n",
    "\n",
    "# \n",
    "# get topk keyword function\n",
    "def get_top_words():\n",
    "    top_cate_words={} # final result\n",
    "    counter_all = Counter() # counter for category '全部'\n",
    "    for category in news_categories:\n",
    "\n",
    "        df_group = df[df.category == category]\n",
    "\n",
    "        # concatenate all filtered words in the same category\n",
    "        words_group = []\n",
    "        for row in df_group.token_pos:\n",
    "\n",
    "            # filter words for each news\n",
    "            filtered_words =[]\n",
    "            for (word, pos) in eval(row):\n",
    "                if (len(word) >= 2) & (pos in allowedPOS):\n",
    "                    filtered_words.append(word)\n",
    "\n",
    "            # concatenate filtered words  \n",
    "            words_group += filtered_words\n",
    "\n",
    "        # now we can count word frequency\n",
    "        counter = Counter( words_group )\n",
    "\n",
    "        # counter \n",
    "        counter_all += counter\n",
    "        topwords = counter.most_common(100)\n",
    "\n",
    "        # store topwords\n",
    "        top_cate_words[category]= topwords\n",
    "\n",
    "    # Process category '全部'\n",
    "    top_cate_words['全部'] = counter_all.most_common(100)\n",
    "    \n",
    "    # To conveniently save data using pandas, we should convert dict to list.\n",
    "    return list(top_cate_words.items())\n",
    "\n",
    "# Save top 200 word frequency for each category\n",
    "top_group_words = get_top_words()\n",
    "df_top_group_words = pd.DataFrame(top_group_words, columns = ['category','top_keys'])\n",
    "df_top_group_words.to_csv('cna_news_topkey_with_category_via_token_pos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
